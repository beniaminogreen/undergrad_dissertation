\documentclass{article}

\usepackage{setspace}
\usepackage[colorlinks, allcolors=blue]{hyperref}
\usepackage{graphicx}
\usepackage{pdfpages}
\usepackage{float}
\usepackage{chngpage}
\usepackage{mathtools}
\usepackage{subcaption}
\usepackage{framed}
\usepackage{listings}
\usepackage{amssymb}
\usepackage{tikz}
\usetikzlibrary{positioning,arrows.meta}

\definecolor{arrowblue}{RGB}{98,145,224}

\newcommand\ImageNode[3][]{
  \node[draw=arrowblue!80!black,line width=1pt,#1] (#2) {\includegraphics[width=3.5cm,height=3.5cm]{#3}};
}
\graphicspath{ {./media/} }
\usepackage[backend=biber,natbib, style=authoryear-icomp,doi=false,isbn=false,url=false]{biblatex}
\addbibresource{bib.bib}

\definecolor{codegreen}{rgb}{0,0.6,0}
\definecolor{codegray}{rgb}{0.5,0.5,0.5}
\definecolor{codepurple}{rgb}{0.58,0,0.82}
\definecolor{backcolour}{rgb}{0.95,0.95,0.92}

\lstdefinestyle{mystyle}{
    backgroundcolor=\color{backcolour},
    commentstyle=\color{codegreen},
    keywordstyle=\color{magenta},
    numberstyle=\scriptsize\color{codegray},
    stringstyle=\color{codepurple},
    basicstyle=\ttfamily\scriptsize,
    breakatwhitespace=false,
    breaklines=true,
    captionpos=b,
    keepspaces=true,
    numbers=left,
    numbersep=5pt,
    showspaces=false,
    showstringspaces=false,
    showtabs=false,
    tabsize=2
}
\lstset{style=mystyle}

\addtolength{\oddsidemargin}{-.5in}
\addtolength{\evensidemargin}{-.5in}
\addtolength{\textwidth}{1in}
\addtolength{\topmargin}{-.5in}
\addtolength{\textheight}{1in}

\newcommand{\wone}{[Word 1] }
\newcommand{\wones}{[Word 1]'s }
\newcommand{\woneps}{[Word 1](s) }

\newenvironment{centerfig}
{\begin{figure}[H]\centering}
{\end{figure}}


<<echo=FALSE, warn=FALSE, include=FALSE>>=
# Set options to knit this document, load libraries
knitr::opts_chunk$set(out.width="100%", message=F, cache=T,echo=F, warn=F, include=F)
library(tidyverse)
library(sf)
library(broom)
library(stargazer)
@

\begin{document}
\title{Sinclair Broadcasting and Racial Resentment: Evidence from Google Trends}
\maketitle{}

\newpage
\section*{Aknowledgements}
\newpage
\section*{Abstract}
\newpage



\tableofcontents

\newpage

\textit{This paper uses Google search terms containing offensive language as a proxy measure for racial animus. Following the practice of similar works \parencites{Stephens_Davidowitz_2014}{Chae_2015}{Chae_2018}{Isoya_2021}, I use coded language to refer to these terms. The words themselves can be found in Table \ref{words}}

\section{Introduction}

Can media coverage influence racial resentment?
Empirical studies suggest that when white Americans understand welfare policies to threaten their privileged status in the U.S. Social hierarchy, their resentment of minorities increases and their support for welfare decreases \parencites[][]{Willer_2016}{Wetts_2018}.
This tendency seems to be weaponed by conservative media institutions and politicians, who seem to encourage racial animus to erode support for social programs, as in the infamous example of the ``Welfare Queen" narrative, a racial stereotype employed to undercut support for the Aid to Families with Dependent Children (AFDC).
However, the link between traditional media coverage and racial resentment has not been extensively studied.

I propose a study exploiting the expansion of the Sinclair Broadcasting Group from 2004-2021 to understand how conservative media messaging impacts racial resentment in a media market.
The expansion of the Sinclair Broadcasting Group during this period provides the basis for a difference-in-differences analysis estimating the effect of Sinclair Media purchasing a station on racial animus in an area.
I use the concentration of racially charged Google searches a proxy for the intensity of racial animus in an area.
This measure has previously been used to measure the contributions of racial resentment towards African-American mortality \parencites{Chae_2018}{Chae_2015}, election outcomes \parencite{Stephens_Davidowitz_2014}, and economic inequality \parencite[][]{Connor_2019}, and does not suffer from the same social censoring issues that confound traditional measures.

\section{Theoretical Framework}

\section{Background}
\subsection{Sinclair Broadcast Group}

During the period of 2004-2021, the Sinclair Broadcasting Group bought or sold stations in 67 media markets.
I use this pattern of expansion as the basis for a difference-in-differences analysis.

I extract a record of which markets Sinclair has a presence in from the company's yearly filings with the Securities and Exchange Commission, which report each station owned by the company at the end of the time of filing a map of which regions Sinclair moved into / out of can be seen in figure \ref{map}.
This strategy has previously been used by \citet{Miho_2018} to track Sinclair Expanison over the period of 1995-2017.
SEC filings have an advantage over the typical sets data used to track network expansion maintained by the Nielsen Corporation as they are publicly available, so the findings can be easily reproduced.

\subsection{Does Sinclair Have a Racial Bias?}

This analysis assumes that Sinclair stations differ from non-Sinclair stations on coverage of racial issues: if Sinclair stations deploy the same coverage on racial issues as other stations, then we would expect to see no difference in levels of racial resentment between Sinclair and non-Sinclair media markets.
So, it must be asked: does Sinclair coverage have a racially-conservative bent?
Admittedly, there have been no large-scale analyses of the effects of Sinclair ownership on coverage of racial issues.
Nonetheless, I submit that there is evidence to show Sinclair Stations cover racial issues in a more conservative light than they would absent Sinclair ownership.

Previous research has demonstrated that Sinclair acquisition of a network is associated with a sharp rightwards shift in its coverage \parencite[][]{Martin_2019}.
I argue this effect extends to its framing of racial issues.

This year, the Sinclair corporation drew ire for a series of ``must-run'' segments on police violence following the murder of George Floyd pushing the ``black-on-black violence'' canard and advocating for a military response to the protests \parencites{Pleat_2020_b}{Pleat_2020_a}.

In analyzing the effects of news coverage on racial animus, it might be natural to examine the expansion of Fox News, which occupies a position in the public consciousness as among the most conservative stations on racial issues, and is the most trusted media outlet among Republican and Republican-leaning respondents in many polls \parencite[][]{Jurkowitz_2020}.
However, I choose to use Sinclair over Fox News Stations for two reasons.
First, Fox News' expansion strategy has involved purchasing a larger stations.
As television companies in the US can only expand until they broadcast to 39\% of U.S. households \parencite[][]{Scherer_2018}, Fox News has been able to buy fewer stations than Sinclair, which entails a smaller sample size of stations that changed ownership.
Second, Fox News' expansion primarily happened before 2004, the first year for which Google Trends data is available, which further limits the sample size of stations which changed ownership for which there is data on racial animus.

\subsection{Google Trends Data}

I use the concentration of Google searches containing racial epithets as a proxy measurement of the level of racial animus in an area.
Data from Google searches are ideal are searches not subject to the social-desirability biases that confound traditional, survey-based measures of racial resentment, and provide a large, regularly-sampled source of data which represents a great deal of the population (at present, Google as an over 85\% market share in the US).

The core assumption of this strategy is that Google searches for the word \wone reflect underlying racial animus in an area.
If readers find this assumption unconvincing, then they will have little reason to accept the conclusions of this analysis.
Accordingly, I offer several reasons to suggest that searches for racial epithets well proxy racial animus in an area, namely: searches are conducted in private, and are not subject to the same social-desirability biases as traditional, survey-based measures; and measurements of area-level racism obtained from Google trends data correspond well to traditional survey-based and non-survey-based measures of racial animus.

First, Google searches do not suffer from the same social censoring as traditional measures of public opinion that rely on face-to-face or over-the-phone interviews.
Overt expressions of racism are no longer socially palatable in the U.S.,
Social desirability bias is a pressing concern when it comes to measuring racial animus; research has repeatedly demonstrated that \parencite[][]{Kuklinski_1997}

This measurement approach has an advantage over traditional survey-based measures of racial animus in that it is less subject to a social desirability bias;
``Google searchers are online and likely alone, both of which make it easier to express socially taboo thoughts (Kreuter et al., 2009)" \parencite[][26]{Stephens_Davidowitz_2014}.
Further, it provides a high-resolution set of data that would be prohibitively expensive to collect from a traditional survey, especially given that the difference-in-differences approach requires a repeated survey comparable across multiple time periods.

\footnote{\cite{Stephens_Davidowitz_2014} evidences this claim by reporting statistics for pornography searches. Over the past 16 years, the number of searches for ``porn" and ``news" are commensurate, yet only 14\% of GSS respondents tell the GSS they have visited a pornographic website in the past 30 days.}

As such, I give several reasons to suggest that Google trends data can be used to measure racial animus.

%Originally developed by \citet{Stephens_Davidowitz_2014} to understand the link between racial animus and the under-performance of black candidates in national elections.
%This measure has significant advantages over traditional survey-based methods as it is not subject to the same social desirability biases that confound traditional measures; and provides a rich, regularly-sampled repeated measure of racial animus.



<<include=T, warn=FALSE>>=
library(ggrepel)

state_iat_data <- read_csv("../data/iat_state_data.csv")
word1_state_data <- read_csv("../data/word1_all_time.csv")

state_iat_searches <- full_join(state_iat_data,word1_state_data) %>%
    mutate(sbp5 = 5 - sbp5) %>%
    drop_na()
@

\begin{figure}
    \makebox[\linewidth][c]{%
        \begin{subfigure}[bt]{.6\textwidth}
            \centering
<<include=T>>=
ggplot(state_iat_searches, aes(word1, iat, label=state)) +
    geom_text_repel() +
    geom_smooth(method="lm") +
    xlab("Frequency of Google Searches for [Word 1]") +
    ylab("Scaled IAT Score of White Respondents")
@
        \end{subfigure}%
    \begin{subfigure}[bt]{.6\textwidth}
        \centering
<<include=T>>=
ggplot(state_iat_searches, aes(word1, sbp5, label=state)) +
    geom_text_repel() +
    geom_smooth(method="lm") +
    xlab("Frequency of Google Searches for [Word 1]") +
    ylab("Scaled Responses to Prompt 5")
@
    \end{subfigure}}\\
\end{figure}


A pressing concern is that Google searches for ``\woneps'' may not actually capture the extent of racial bias in an area, but simply reflect users learning about the term.
In fact, ``definition of \wone'' and ``what does \wone mean" are both among the top 5 search queries related to the ``\wone".
These queries suggest that many who search for the term are searching out of curiosity to investigate the term.

This is not to suggest that searches for the term do not capture any variation in racial animus: among the top 10 most searched related queries to the term \wones are ``I don't like \wones,"  ``fuck the \wones," and ``ship those \wones back."

The concern that Google Searches for racial slurs may largely reflect curiosity about the term is valid.
Subject to data availability, I suggest controlling for the frequency of searches for the definitions of these terms in each area, to try and isolate the effect of Sinclair media entering a market on searches expressing ``hardcore" racial animus rather than curiosity.


\subsection{Scaling Google Trends Data:}

Google trends data measures the popularity of a search on an idiosyncratic scale: a term's ``search score" in a given time is given as the volume of searches over that time divided by the volume of searches when the term was most popular.
In this section, I describe the scaling process used to back-out an interval-level measure of search popularity from these search scores.

First, a full explanation of the problem.
Search popularity can be obtained from Google in two flavors: a measure that compares the popularity of a search across all regions at a given time, and a measure that gives the popularity of a search in a given region across all times.

The search score between regions at a given time is defined as the following:
\[
    \text{Between Regions Search Score} = \frac{\text{Popularity in Region } i }{\text{Popurality in the Most Popular Region}}
\]

The search score that can be used to compare the popularity across different times in a given region is defined as the following.
\[
    \text{Between Times Search Score} = \frac{\text{Popularity at Time } i }{\text{Popurality at the Most Popular Time}}
\]

The first measure allows for comparisons between regions but not between times, while the second permits comparisons between times but only within one region.

\section{Methods}

\subsection{Tools Used}

%Surveys of major social science journals routinely fail to reproduce the findings of a plurality or majority of papers from the supplementary code and data provided \parencites[][]{Nuijten_2015}{Nuijten_2020}{Eubank_2016}.
%Failures to replicate are often due to coding errors or mistakes in transcribing the results of a calculation into a published manuscript \parencite[][276]{Eubank_2016}.

I use Kintr to integrate statistical calculations into the paper, eliminating the possibility of transcription errors \parencite[][]{knitr}.
To ensure that the methods of this paper have been properly implemented and the finding are reproducible, I tested the analysis routines using the \textit{testthat} package in R \parencite[][]{testthat} and the \textit{unittest} module in Python \parencite[][]{Python}.  But I won't make you take my word for it  –
I provide a Docker image with the reproducibility materials to ensure others can replicate the calculations on their own systems \parencites{docker}{Boettiger_2015}.
The net result is ``one-click reproducibility" \parencite[][]{N_st_2020}; readers can reproduce this exact paper with the push of a button from the linked materials.
\footnote{Replication materials available \href{https://github.com/beniaminogreen/dissertation}{here}. By default, the webscraping and models using IAT data are not re-run as they are memory and time-intensive.}

\subsection{Preregistration}

To avoid the possibility of fitting hypotheses to the data after results are known, I created a preregistration plan of my analysis using the Google trends data. The plan can be seen in section \ref{prereg}.
As I decided to conduct the analysis using the project implicit data to confirm the Google trends results, I did not preregister this analysis plan.

I have made just one deviation from the preregistration.
In my preregistration, I describe a different strategy to scale the Google trends data than the one I actually employ.
This original strategy is based on a misunderstanding of the format of Google trends data, and does not actually produce the desired measure.

In the analysis I perform, I correct this mistake. I describe the correct scaling procedure in section \ref{hey}.


\begin{figure}
\caption{Sinclair News Anchors Reading a ``Must-Run'' Script (May 2018)}
\include{media/must_run.tex}
\end{figure}


\begin{figure}
\caption{Illustration of Scaling Algorithm}
\include{media/scaling.tex}
\end{figure}

\begin{figure}
    \makebox[\linewidth][c]{%
        \begin{subfigure}[bt]{.6\textwidth}
            \centering
<<>>=
# Load and create data to show markets Sinclair has expanded to / moved out of
sinclair_expansions <- read_csv("../data/clean_sinclair_data.csv") %>%
    nest(-code) %>%
    mutate(
           any_true =  data %>% map_lgl(~any(.$sinclair_present)),
           any_false =  data %>% map_lgl(~any(!.$sinclair_present)),
           changed = any_true & any_false
    )


dma_boundaries <- st_read("../data/dma_boundaries/dma_boundary.shp")
dma_boundaries <- merge(dma_boundaries, sinclair_expansions, by.x="dma0", by.y="code")
@

\caption{Media Markets Sinclair Bought or Sold a Station in between 2004-2020. DMA Boundaries From \Cite[][]{Hill_2015}}
\label{map}
<<include = T>>=
#Plot markets Sinclair has expanded to / moved out of
ggplot() +
    geom_sf(data=dma_boundaries, aes(fill=changed), alpha=.5) +
    scale_fill_manual(values  = c(NA, "blue")) +
    theme_void() +
    theme(legend.pos ="none")
@
        \end{subfigure}%
    \begin{subfigure}[bt]{.6\textwidth}
        \centering
\caption{Media Markets by Sinclair Ownership Status, Time}
<<include = T, warn = F >>=
#Plot markets Sinclair has expanded to / moved out of
sinclair_expansions %>%
    unnest(cols=c(data)) %>%
    unite("fill", changed:sinclair_present, remove = F)  %>%
    mutate(code = fct_reorder2(as.factor(code),as.factor(changed),as.factor(sinclair_present))) %>%
    ggplot(aes(year,code, fill=sinclair_present)) +
    geom_raster() +
    scale_y_discrete(breaks=c()) +
    scale_fill_discrete(labels = c("Untreated", "Treated")) +
    theme(legend.title = element_blank()) +
    xlab("Year") +
    ylab("Media Market")
@
    \end{subfigure}}\\
\end{figure}

<<>>=
full_data <- read_csv("../data/full_data.csv") %>%
    mutate(code = as.factor(code))

full_data %>%
    nest(-code) %>%
    mutate(
           any_true =  data %>% map_lgl(~any(.$sinclair_present)),
           any_false =  data %>% map_lgl(~any(!.$sinclair_present)),
           changed = any_true & any_false
    ) %>%
    unnest()  %>%
    arrange(changed,years_before) %>%
    mutate(group_num = as.integer(factor(code, levels = unique(.$code)))) %>%
    ungroup() %>%
    mutate(col = interaction(changed,sinclair_present)) %>%
ggplot(aes(x=year,y=group_num,fill=col)) +
geom_raster()
@


\section{Results}

<<>>=

model_1 <- lm(sword1 ~ as.factor(year) + code + sinclair_present, data = full_data)

model_2 <- lm(sword1 ~ as.factor(year) + code + code:year+ sinclair_present, data = full_data)

model_3 <- lm(sword1 ~ as.factor(year) + as.factor(code) + as.factor(years_before), data = full_data)

@


\[
    \text{Racially Charged Search Rate}= \beta_{1}(\text{ Sinclair Present }) + \beta_{2}(\text{ DMA fixed effects }) + \beta_{3}(\text{ year fixed effects })
\]



<<include=TRUE, results="asis">>=
stargazer(model_1, model_2,
          omit=c('^as\\.factor\\(year\\)[0-9]{4}$',
                 "^code[0-9]{3}$",
                 "^code[0-9]{3}\\:year$"
                 ),
	title = "Fixed-Effect Model Results",
	dep.var.labels = c("Frequency of Google Searches for \\wone"),
	covariate.labels = c("Sinclair Present", "Constant"),
	add.lines =list(c("Year Fixed Effects", "Yes", "Yes"),c("Region Fixed Effects", "Yes", "Yes"), c("Region Time Trends", "No", "Yes")),
           table.placement="H")
@

\subsection{Indentification Asssumption}

In this section, I test the indefication assumption, the assumption that the treated and control units would have the outcomes if the treatment were absent.


\[
    \text{Racially Charged Search Rate} = \beta_{1}(\text{ Sinclair Present }) + \beta_{2}(\text{ DMA fixed effects }) + \beta_{3}(\text{ year fixed effects }) +
    \beta_{4}(\text{Year / DMA fixed effects})
\]

\begin{centerfig}
\caption{Fixed-Effects Estimates of the effect of Sinclair Acquisition on Rate of Racially Charged Google Searches}
<<include =T, warn = FALSE >>=
model_3 %>%
  tidy() %>%
  na.omit() %>%
  filter(grepl("years_before", term)) %>%
  mutate(term = as.numeric(gsub("[^0-9\\-]+", "", term))) %>%
  ggplot(aes(x = term, y = estimate)) +
  geom_point() +
  geom_errorbar(aes(ymin = estimate - 1.96 * std.error, ymax = estimate + 1.96 * std.error)) +
  geom_hline(aes(yintercept=0), linetype=2) +
  geom_vline(aes(xintercept=0)) +
  xlab("Years Relative to Sinclair Acquisition") +
  ylab("Estimated Effect of Acquisition on Number of Racially-Charged Google Searches")
@
\end{centerfig}



\newpage
\appendix{}
\section{Codings for Offensive Words}
\begin{figure}
\begin{table}[H]
\centering
        \begin{tabular}{l|l}
\label{words}
            Code & Word \\ \hline
            Word 1  & nigger   \\ \hline
        \end{tabular}
\end{table}
\end{figure}
\printbibliography{}

\section{Code In This Document}
<<echo=FALSE, include=T>>=
# Pull R code out of this document to put into the appendix
code <- knitr::purl("diss.Rnw", quiet=T)
@

\section{Web Scraping Code}
\subsection{Master Web Scraping Script}
\lstinputlisting[language=python]{../code/web_scrape.py}
\subsection{Collecting Data for Between-Region Comparisons}
\lstinputlisting[language=python]{../code/between_regions.py}
\subsection{Collecting Within-Region Data}
\lstinputlisting[language=python]{../code/in_region.py}
\subsection{Utility Functions}
\lstinputlisting[language=python]{../code/utils.py}
\section{Analysis Code}
\lstinputlisting[language=R]{../code/analysis.R}
\subsection{Utility Functions}
\lstinputlisting[language=R]{../code/utils.R}
\section{Unit tests}
\subsection{For Python Code}
\lstinputlisting[language=python]{../code/test_between_regions.py}
\lstinputlisting[language=python]{../code/test_in_region.py}
\lstinputlisting[language=python]{../code/test_scaling.py}
\lstinputlisting[language=python]{../code/test_utils.py}
\subsection{For R Code}
\lstinputlisting[language=R]{../code/tests/testthat/test_utils.R}

\end{document}
