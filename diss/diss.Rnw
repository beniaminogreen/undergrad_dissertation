\documentclass{article}

\usepackage{setspace}
\usepackage{relsize}
\usepackage[colorlinks, allcolors=blue]{hyperref}
\usepackage{graphicx}
\usepackage{pdfpages}
\usepackage{float}
\usepackage{chngpage}
\usepackage{mathtools}
\usepackage{subcaption}
\usepackage{framed}
\usepackage{listings}
\usepackage{amssymb}
\usepackage{tikz}
\usetikzlibrary{positioning,arrows.meta}

\definecolor{arrowblue}{RGB}{98,145,224}

\newcommand\ImageNode[3][]{
  \node[draw=arrowblue!80!black,line width=1pt,#1] (#2) {\includegraphics[width=3.5cm,height=3.5cm]{#3}};
}
\graphicspath{ {./media/} }
\usepackage[backend=biber,natbib, style=authoryear-icomp,doi=false,isbn=false,url=false]{biblatex}
\addbibresource{bib.bib}

\definecolor{codegreen}{rgb}{0,0.6,0}
\definecolor{codegray}{rgb}{0.5,0.5,0.5}
\definecolor{codepurple}{rgb}{0.58,0,0.82}
\definecolor{backcolour}{rgb}{0.95,0.95,0.92}

\lstdefinestyle{mystyle}{
    backgroundcolor=\color{backcolour},
    commentstyle=\color{codegreen},
    keywordstyle=\color{magenta},
    numberstyle=\scriptsize\color{codegray},
    stringstyle=\color{codepurple},
    basicstyle=\ttfamily\scriptsize,
    breakatwhitespace=false,
    breaklines=true,
    captionpos=b,
    keepspaces=true,
    numbers=left,
    numbersep=5pt,
    showspaces=false,
    showstringspaces=false,
    showtabs=false,
    tabsize=2
}
\lstset{style=mystyle}

\addtolength{\oddsidemargin}{-.5in}
\addtolength{\evensidemargin}{-.5in}
\addtolength{\textwidth}{1in}
\addtolength{\topmargin}{-.5in}
\addtolength{\textheight}{1in}

\newcommand{\wone}{[Word 1] }
\newcommand{\wones}{[Word 1]'s }
\newcommand{\woneps}{[Word 1](s) }

\newenvironment{centerfig}
{\begin{figure}[H]\centering}
{\end{figure}}


<<echo=FALSE, warn=FALSE, include=FALSE>>=
# Set options to knit this document, load libraries
knitr::opts_chunk$set(out.width="100%", message=F, cache=F,echo=F, warn=F, include=F)
library(tidyverse)
library(sf)
library(broom)
library(stargazer)
@

\begin{document}
\title{Sinclair Broadcasting and Racial Resentment: Evidence from Google Trends and Project Implicit}
\maketitle{}

\newpage
\section*{Acknowledgments}
\newpage
\section*{Abstract}
\newpage

\tableofcontents

\newpage

\textit{This paper uses Google search terms containing offensive language as a proxy measure for racial animus. Following the practice of similar works \parencites{Stephens_Davidowitz_2014}{Chae_2015}{Chae_2018}{Isoya_2021}, I use coded language to refer to these terms. The words themselves can be found in Table \ref{words}}

\section{Introduction}

Can media coverage influence racial resentment?
Empirical studies suggest that when white Americans understand welfare policies to threaten their privileged status in the U.S. Social hierarchy, their resentment of minorities increases and their support for welfare decreases \parencites[][]{Willer_2016}{Wetts_2018}.
This tendency seems to be weaponed by conservative media institutions and politicians, who seem to encourage racial animus to erode support for social programs, as in the infamous example of the ``Welfare Queen" narrative, a racial stereotype employed to undercut support for the Aid to Families with Dependent Children (AFDC).
However, the link between traditional media coverage and racial resentment has not been extensively studied.

I use the expansion of the Sinclair Broadcasting Group from 2004-2021 to understand how conservative media messaging impacts racial resentment in a media market.
The expansion of the Sinclair Broadcasting Group during this period provides the basis for a difference-in-differences analysis estimating the effect of Sinclair Media purchasing a station on racial animus in an area.

I use the number of Google searches containing racial epithets in an area as a proxy of the racial animus in an area.
This measure has previously been used to measure the contributions of racial resentment towards African-American mortality \parencites{Chae_2018}{Chae_2015}, election outcomes \parencite{Stephens_Davidowitz_2014}, and economic inequality \parencite[][]{Connor_2019}, and does not suffer from the same social censoring issues that confound traditional measures.

Using two internet-based measures of racial animus, I show that Sinclair moving into a region has no effect on the level of racial bias in an area.


\section{Theoretical Framework}

\section{Background}
\subsection{Sinclair Broadcast Group}

Sinclair Broadcasting group is one of the largest telecommunications operators in the United States.  Founded in 1971 as the Chesapeake Television Corporation, Sinclair Broadcasting group has since expanded to reach the maximum 39\% of total houseolds any operator can service under U.S. law.
In this paper, I use the variation in station ownership caused by Sinclair's expansion in the period of 2004-2021 as the basis of a difference-in-differences analysis.

I create a record of Sinclair's station ownership over time from the company's yearly filings with the Securities and Exchange Commission, which enumerate all the stations owned by the company at the end of each year.
This strategy has previously been used by \citet{Miho_2018} to track Sinclair Expanison over the period of 1995-2017.
A map of the media markets Sinclair either purchased or sold a station in can be seen in in figure \ref{map}. A chart showing when each station became treated can be seen in \ref{treated}.

Traditionally, researchers track a network's expansion using datasets maintained by the Nielsen Corporation, the U.S. company responsible for tracking the boundaries of media markets.
The SEC dataset has an advantage over this dataset as it is open-access, and can be published freely alongside the research.

\begin{figure}
    \makebox[\linewidth][c]{%
        \begin{subfigure}[bt]{.6\textwidth}
            \centering
<<>>=
# Load and create data to show markets Sinclair has expanded to / moved out of
sinclair_expansions <- read_csv("../data/clean_sinclair_data.csv") %>%
    nest(-code) %>%
    mutate(
           any_true =  data %>% map_lgl(~any(.$sinclair_present)),
           any_false =  data %>% map_lgl(~any(!.$sinclair_present)),
           changed = any_true & any_false
    )


dma_boundaries <- st_read("../data/dma_boundaries/dma_boundary.shp")
dma_boundaries <- merge(dma_boundaries, sinclair_expansions, by.x="dma0", by.y="code")
@

\caption{Media Markets Sinclair Bought or Sold a Station in between 2004-2020. DMA Boundaries From \Cite[][]{Hill_2015}}
\label{map}
<<include = T>>=
#Plot markets Sinclair has expanded to / moved out of
ggplot() +
    geom_sf(data=dma_boundaries, aes(fill=changed), alpha=.5) +
    scale_fill_manual(values  = c(NA, "blue")) +
    theme_void() +
    theme(legend.pos ="none")
@
        \end{subfigure}%
    \begin{subfigure}[bt]{.6\textwidth}
        \centering
\caption{Media Markets by Sinclair Ownership Status, Time}
\label{treated}
<<include = T, warn = F >>=
#Plot markets Sinclair has expanded to / moved out of
sinclair_expansions %>%
    unnest(cols=c(data)) %>%
    unite("fill", changed:sinclair_present, remove = F)  %>%
    mutate(code = fct_reorder2(as.factor(code),as.factor(changed),as.factor(sinclair_present))) %>%
    ggplot(aes(year,code, fill=sinclair_present)) +
    geom_raster() +
    scale_y_discrete(breaks=c()) +
    scale_fill_discrete(labels = c("Untreated", "Treated")) +
    theme(legend.title = element_blank()) +
    xlab("Year") +
    ylab("Media Market")
@
    \end{subfigure}}\\
\end{figure}

\subsubsection{Does Sinclair Have a Racial Bias?}

Previous quantitative research has shown that when Sinclair buys a station, its coverage shifts sharply rightwards.
I make the case that this effect extends to the coverage of racial issues - when Sinclair buys a station, it shifts the coverage of racial issues sharply towards the right.

assume that Sinclair stations differ from non-Sinclair stations on coverage of racial issues: if Sinclair stations deploy the same coverage on racial issues as other stations, then we would expect to see no difference in levels of racial resentment between Sinclair and non-Sinclair media markets.

So, it must be asked: does Sinclair coverage have a racially-conservative bent?
Admittedly, there have been no large-scale analyses of the effects of Sinclair ownership on coverage of racial issues.
Nonetheless, I submit that there is evidence to show Sinclair Stations cover racial issues in a more conservative light than they would absent Sinclair ownership.

Previous research has demonstrated that Sinclair acquisition of a network is associated with a sharp rightwards shift in its coverage \parencite[][]{Martin_2019}.
I argue this effect extends to its framing of racial issues.

This year, the Sinclair corporation drew ire for a series of ``must-run'' segments on police violence following the murder of George Floyd pushing the ``black-on-black violence'' canard and advocating for a military response to the protests \parencites{Pleat_2020_b}{Pleat_2020_a}.

% In analyzing the effects of news coverage on racial animus, it might be natural to examine the expansion of Fox News, which occupies a position in the public consciousness as among the most conservative stations on racial issues, and is the most trusted media outlet among Republican and Republican-leaning respondents in many polls \parencite[][]{Jurkowitz_2020}.
% However, I choose to use Sinclair over Fox News Stations for two reasons.
% First, Fox News' expansion strategy has involved purchasing a larger stations.
% As television companies in the US can only expand until they broadcast to 39\% of U.S. households \parencite[][]{Scherer_2018}, Fox News has been able to buy fewer stations than Sinclair, which entails a smaller sample size of stations that changed ownership.
% Second, Fox News' expansion primarily happened before 2004, the first year for which Google Trends data is available, which further limits the sample size of stations which changed ownership for which there is data on racial animus.

\subsection{Measuring Racial Animus}

Measuring racial resentment is difficult.
Overt expressions of racism are severly socially sanctioned in the U.S., so respondents who do harbor racial resentment often understate it when they are interviewed for traditional surveys \parencites{Krumpal_2011}{Kuklinski_1997}.
These social-desirability biases confound traditional survey measures, and lead to underestimates of the levels of racial hostility in the states.

% The Google search results provide higher external validity (almost all of the population is represented in Google search data) at the cost of low power (the data is variable, so small effects would be difficult or impossible to detect).
% I compliment this with data from Project Implicit which trades on external validity (IAT takers do not represent the general population) but the large sample size offers high power, so the chance of not finding a small effect is low.

I sidestep social desirability biases by using two unconventional measures of racial animus: the number of Google searches for racial slurs in an area, and test scores from Harvard University's Project Implicit, an internet-based project to collect data on implicit biases.
Both measures estimate racial animus from observed behavior (Google searches or the ability to associate black and white faces with positive words) rather than asking respondents questions, so sidestep issues of self-reporting.

\subsection{Google Trends Data}

I use the concentration of Google searches containing racial epithets as a proxy measurement of the level of racial animus in an area.
Data from Google searches are ideal are searches not subject to the social-desirability biases that confound traditional, survey-based measures of racial resentment, and provide a large, regularly-sampled source of data which represents a great deal of the population (at present, Google as an over 85\% market share in the US).

The core assumption of this strategy is that Google searches for the word \wone reflect underlying racial animus in an area.
If readers find this assumption unconvincing, then they will have little reason to accept the conclusions of this analysis.
Accordingly, I offer several reasons to suggest that searches for racial epithets well proxy racial animus in an area, namely: searches are conducted in private, and are not subject to the same social-desirability biases as traditional, survey-based measures; and measurements of area-level racism obtained from Google trends data correspond well to traditional survey-based and non-survey-based measures of racial animus.

First, Google searches do not suffer from the same social censoring as traditional measures of public opinion that rely on face-to-face or over-the-phone interviews.
Overt expressions of racism are no longer socially palatable in the U.S.,
Social desirability bias is a pressing concern when it comes to measuring racial animus; research has repeatedly demonstrated that \parencite[][]{Kuklinski_1997}

This measurement approach has an advantage over traditional survey-based measures of racial animus in that it is less subject to a social desirability bias;
``Google searchers are online and likely alone, both of which make it easier to express socially taboo thoughts (Kreuter et al., 2009)" \parencite[][26]{Stephens_Davidowitz_2014}.
Further, it provides a high-resolution set of data that would be prohibitively expensive to collect from a traditional survey, especially given that the difference-in-differences approach requires a repeated survey comparable across multiple time periods.
\footnote{\cite{Stephens_Davidowitz_2014} evidences this claim by reporting statistics for pornography searches. Over the past 16 years, the number of searches for ``porn" and ``news" are commensurate, yet only 14\% of GSS respondents tell the GSS they have visited a pornographic website in the past 30 days.}

%As such, I give several reasons to suggest that Google trends data can be used to measure racial animus.

%Originally developed by \citet{Stephens_Davidowitz_2014} to understand the link between racial animus and the under-performance of black candidates in national elections.
%This measure has significant advantages over traditional survey-based methods as it is not subject to the same social desirability biases that confound traditional measures; and provides a rich, regularly-sampled repeated measure of racial animus.

A pressing concern is that searches for  the word ``\wone'' might not actually measure racial animus but, in fact, reflect users learning about the term.
Indeed, ``Definition of \wone'' and ``what does \wone mean" are both among the top 5 search queries related to term.
These queries suggest that many who search for the term are searching out of curiosity to investigate the term.

This is not to suggest that searches for the term do not capture any variation in racial animus: the top 10 most searched related queries to the term \wones are ``I don't like \wones,"  ``fuck the \wones," and ``ship those \wones back."
\citet{Stephens_Davidowitz_2014} evidences this claim by reporting strong correlations between opposition to interracial marriage and Google searches for \wone at the state level.
I provide further support for this claim by reporting correlations between searches for \wone and the scores of white respondents in a state on the IAT.
Following practices of other research using IAT data \parencite[][206]{Connor_2019} I use the scores of only white respondents.


<<include=T, warn=FALSE>>=
library(ggrepel)

state_iat_data <- read_csv("../data/iat_state_data.csv")
word1_state_data <- read_csv("../data/word1_all_time.csv")

state_iat_searches <- full_join(state_iat_data,word1_state_data) %>%
    mutate(sbp5 = 5 - sbp5) %>%
    drop_na()
@


\begin{figure}
\ref{correlations}
    \makebox[\linewidth][c]{%
        \begin{subfigure}[bt]{.6\textwidth}
            \centering
<<include=T>>=
ggplot(state_iat_searches, aes(word1, iat, label=state)) +
    geom_text_repel() +
    geom_smooth(method="lm") +
    xlab("Frequency of Google Searches for [Word 1]") +
    ylab("Scaled IAT Score of White Respondents")
@
        \end{subfigure}%
    \begin{subfigure}[bt]{.6\textwidth}
        \centering
<<include=T>>=
ggplot(state_iat_searches, aes(word1, sbp5, label=state)) +
    geom_text_repel() +
    geom_smooth(method="lm") +
    xlab("Frequency of Google Searches for [Word 1]") +
    ylab("Scaled Responses to Prompt 5")
@
    \end{subfigure}}\\
\end{figure}

\subsection{Scaling Google Trends Data:}

Google trends data measures the popularity of a search on an idiosyncratic scale: a term's ``Google search score" in a given time is given as the volume of searches over that time divided by the volume of searches when the term was most popular.
However, the difference-in-difference analysis I perform requires that the search volume is measured on the same scale.
In this section, I describe the scaling process used to back out an interval-level measure of search popularity from these search scores.

Search popularity can be obtained from Google in two flavors: a measure that compares the popularity of a search across all regions at a given time, and a measure that gives the popularity of a search in a given region across all times.

The search score that can be used to make comparisons between regions at a given time is defined as the following:
\[
    \text{Between Regions Search Score} = \frac{\text{Popularity in Region } i }{\text{Popurality in the Most Popular Region}}
\]

The search score that can be used to compare the popularity across different times in a given region is defined as the following.
\[
    \text{Between Times Search Score} = \frac{\text{Popularity at Time } i }{\text{Popurality at the Most Popular Time}}
\]

The first measure allows for comparisons between regions but not between times, while the second permits comparisons between times but only within one region.
When combined together, these measures can be used to compare the volume of searches across both time periods and regions.

As an example, we might want to compare searches for the word apple in Washington D.C. in 2017 to searches in Pensacola in 2018.
Washington D.C. had twice the number of searches for the word apple as Pensacola in 2017. As Pensacola had three times the amount of searches for the word apple in 2017 as 2018, we know that Pensacola had just $1/6th$ the searches for the word apple in 2018 as Washington D.C. in the year 2017.

By comparing the volume of searches in each region at each time to each other region, I back out a measure of search volume that is comparable both between regions and between times.

\[
    \text{Search Volume in Region } r \text{ at Time } t =
    \mathlarger{\mathlarger{\mathlarger{\sum}}}_{i=1}^{I}
    \mathlarger{\mathlarger{\mathlarger{\sum}}}_{j=1}^{J}
    \frac{\text{Search Volume in Region } r \text{ at Time } t }{\text{Search Volume in Region } i \text{ at Time } j }
\]
With
\begin{itemize}
    \item regions $1\dots J$
    \item time periods $1\dots I$
\end{itemize}



\begin{figure}
\caption{Illustration of Scaling Algorithm}
\include{media/scaling.tex}
\end{figure}

\section{Methods}

\subsection{Tools Used}

I use Knitr \parencite[][]{knitr} to integrate statistical calculations into the paper, eliminating the possibility of transcription errors.
To ensure that the methods the findings are reproducible, I tested the analysis routines using the \textit{testthat} package in R \parencite[][]{testthat} and the \textit{unittest} module in Python \parencite[][]{Python}.
But I won't make you take my word for it â€“ I provide a Docker image with the reproducibility materials to ensure others can replicate the calculations on their own systems \parencites{docker}{Boettiger_2015}.
The net result is ``one-click reproducibility" \parencite[][]{N_st_2020}; readers can reproduce this exact paper with the push of a button from the linked materials. \footnote{Replication materials available \href{https://github.com/beniaminogreen/dissertation}{here}.  By default, the web-scraping does not run, as the data take several days to collect.}

\subsection{Preregistration}

To avoid the possibility of fitting hypotheses to the data after results are known, I created a preregistration plan of my analysis using the Google trends data. The plan can be seen in section \ref{prereg}.
As I decided to conduct the analysis using the project implicit data to confirm the Google trends results, I did not preregister this analysis plan.

I have made one deviation from the preregistration.
In my preregistration, I describe a different strategy to scale the Google trends data than the one I actually employ.
This original strategy is based on a misunderstanding of the format of Google trends data, and does not actually produce the desired measure.

In the analysis I perform, I correct this mistake. I describe the correct scaling procedure in section \ref{hey}.


\begin{figure}
\caption{Sinclair News Anchors Reading a ``Must-Run'' Script (May 2018)}
\include{media/must_run.tex}
\end{figure}



\section{Results}

<<>>=

load("../data/models.Rdata")

@
<<include=TRUE, results="asis">>=
stargazer(model_1, model_2, model_4, model_5,
          omit=c('^as\\.factor\\(year\\)[0-9]{4}$',
                 "^code[0-9]{3}$",
                 "^code[0-9]{3}\\:year$",
                 "code"
                 ),
	title = "Fixed Effects Models For Sinclair Aqusisition on Google Searches",
	dep.var.labels = c("Frequency of Searches for \\wone", "Frequency of Searches for Words 1-5"),
	covariate.labels = c("Sinclair Present", "Constant"),
	add.lines =
        list(
             c("Year Fixed Effects", "Yes", "Yes", "Yes", "Yes"),
             c("Region Fixed Effects", "Yes", "Yes","Yes", "Yes"),
             c("Region Time Trends", "No", "Yes", "No", "Yes")),
        omit.stat=c("LL","ser","f"),
           table.placement="H")
@

<<>>=
library(biglm)
library(texreg)
load("../data/models.Rdata")
@

<<include=TRUE, results="asis">>=
texreg(list(model_7, model_8, model_10, model_11),
       custom.gof.rows =
           list(
                "Year Fixed Effects" = c("Yes", "Yes", "Yes", "Yes"),
                "Region Fixed Effects" = c("Yes", "Yes", "Yes", "Yes"),
                "Region Time Trends" = c("No", "Yes", "No", "Yes"),
                "White Respondents Only" = c("No", "No", "Yes", "Yes")
                ),
       float.pos = "H",
       omit.coef="code|year")
@


\subsection{Identification Assumption}

In this section, I test the indication assumption, the assumption that the treated and control units would have the outcomes if the treatment were absent.


\[
    \text{Racially Charged Search Rate} = \beta_{1}(\text{ Sinclair Present }) + \beta_{2}(\text{ DMA fixed effects }) + \beta_{3}(\text{ year fixed effects }) +
    \beta_{4}(\text{Year / DMA fixed effects})
\]

\begin{centerfig}
\caption{Fixed-Effects Estimates of the effect of Sinclair Acquisition on Searches for ``\wone"}
<<include =T, warn = FALSE >>=
model_3 %>%
  tidy() %>%
  na.omit() %>%
  filter(grepl("years_before", term)) %>%
  mutate(term = as.numeric(gsub("[^0-9\\-]+", "", term))) %>%
  ggplot(aes(x = term, y = estimate)) +
  geom_point() +
  geom_errorbar(aes(ymin = estimate - 1.96 * std.error, ymax = estimate + 1.96 * std.error)) +
  geom_hline(aes(yintercept=0), linetype=2) +
  geom_vline(aes(xintercept=0)) +
  xlab("Years Relative to Sinclair Acquisition") +
  ylab("Estimated Effect of Acquisition on Number of Searches for \"[Word 1]\"")
@
\end{centerfig}

\newpage
\appendix{}
\section{Codings for Offensive Words}
\begin{figure}
\begin{table}[H]
\centering
        \begin{tabular}{l|l}
\label{words}
            Code & Word \\ \hline
            Word 1  & Nigger   \\ \hline
            Word 2 & Coon \\ \hline
            Word 3 & Kike \\ \hline
            Word 4 & Spic \\ \hline
            Word 5 & Spook \\ \hline
        \end{tabular}
\end{table}
\end{figure}
\printbibliography{}

\section{Code In This Document}
<<echo=FALSE, include=T>>=
# Pull R code out of this document to put into the appendix
code <- knitr::purl("diss.Rnw", quiet=T)
@

\section{Web Scraping Code}
% \subsection{Master Web Scraping Script}
% \lstinputlisting[language=python]{../code/web_scrape.py}
% \subsection{Collecting Data for Between-Region Comparisons}
% \lstinputlisting[language=python]{../code/between_regions.py}
% \subsection{Collecting Within-Region Data}
% \lstinputlisting[language=python]{../code/in_region.py}
% \subsection{Utility Functions}
% \lstinputlisting[language=python]{../code/utils.py}
% \section{Analysis Code}
% \lstinputlisting[language=R]{../code/analysis.R}
% \subsection{Utility Functions}
% \lstinputlisting[language=R]{../code/utils.R}
% \section{Unit tests}
% \subsection{For Python Code}
% \lstinputlisting[language=python]{../code/test_between_regions.py}
% \lstinputlisting[language=python]{../code/test_in_region.py}
% \lstinputlisting[language=python]{../code/test_scaling.py}
% \lstinputlisting[language=python]{../code/test_utils.py}
% \subsection{For R Code}
% \lstinputlisting[language=R]{../code/tests/testthat/test_utils.R}

\end{document}
