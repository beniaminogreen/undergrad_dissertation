\documentclass{article}

\usepackage{setspace}
\usepackage{relsize}
\usepackage[colorlinks, allcolors=blue]{hyperref}
\usepackage{graphicx}
\usepackage{pdfpages}
\usepackage{float}
\usepackage{chngpage}
\usepackage{mathtools}
\usepackage{subcaption}
\usepackage{framed}
\usepackage{listings}
\usepackage{amssymb}
\usepackage{tikz}
\usetikzlibrary{positioning,arrows.meta}

\definecolor{arrowblue}{RGB}{98,145,224}

\newcommand\ImageNode[3][]{
  \node[draw=arrowblue!80!black,line width=1pt,#1] (#2) {\includegraphics[width=3.5cm,height=3.5cm]{#3}};
}
\graphicspath{ {./media/} }
\usepackage[backend=biber,natbib, style=authoryear-icomp,doi=false,isbn=false,url=false]{biblatex}
\addbibresource{bib.bib}

\definecolor{codegreen}{rgb}{0,0.6,0}
\definecolor{codegray}{rgb}{0.5,0.5,0.5}
\definecolor{codepurple}{rgb}{0.58,0,0.82}
\definecolor{backcolour}{rgb}{0.95,0.95,0.92}

\lstdefinestyle{mystyle}{
    backgroundcolor=\color{backcolour},
    commentstyle=\color{codegreen},
    keywordstyle=\color{magenta},
    numberstyle=\scriptsize\color{codegray},
    stringstyle=\color{codepurple},
    basicstyle=\ttfamily\scriptsize,
    breakatwhitespace=false,
    breaklines=true,
    captionpos=b,
    keepspaces=true,
    numbers=left,
    numbersep=5pt,
    showspaces=false,
    showstringspaces=false,
    showtabs=false,
    tabsize=2
}
\lstset{style=mystyle}

\addtolength{\oddsidemargin}{-.5in}
\addtolength{\evensidemargin}{-.5in}
\addtolength{\textwidth}{1in}
\addtolength{\topmargin}{-.5in}
\addtolength{\textheight}{1in}

\newcommand{\wone}{[Word 1] }
\newcommand{\wones}{[Word 1]'s }
\newcommand{\woneps}{[Word 1](s) }

\newenvironment{centerfig}
{\begin{figure}[H]\centering}
{\end{figure}}


<<echo=FALSE, warn=FALSE, include=FALSE>>=
# Set options to knit this document, load libraries
knitr::opts_chunk$set(out.width="100%", message=F, cache=F,echo=F, warn=F, include=F)
library(tidyverse)
library(sf)
library(broom)
library(stargazer)
@

\begin{document}
\title{Sinclair Broadcasting and Racial Resentment: Evidence from Google Trends and Project Implicit}
\maketitle{}

\newpage
\section*{Acknowledgments}
\newpage
\section*{Abstract}
\newpage

\tableofcontents

\newpage

\textit{This paper uses Google search terms containing offensive language as a proxy measure for racial animus. Following the practice of similar works \parencites{Stephens_Davidowitz_2014}{Chae_2015}{Chae_2018}{Isoya_2021}, I use coded language to refer to these terms. The words themselves can be found in Table \ref{words}}

\section{Introduction}

Can media coverage influence racial resentment?
Empirical studies suggest that when white Americans understand welfare policies to threaten their privileged status in the U.S. Social hierarchy, their resentment of minorities increases and their support for welfare decreases \parencites[][]{Willer_2016}{Wetts_2018}.
This tendency seems to be weaponed by conservative media institutions and politicians, who encourage racial animus to erode support for social programs, as in the infamous example of the ``Welfare Queen" narrative, a racial stereotype employed to undercut support for the Aid to Families with Dependent Children (AFDC).
However, the link between traditional media coverage and racial resentment has not been extensively studied.

I use the expansion of the Sinclair Broadcasting Group from 2004-2021 to understand how conservative media messaging impacts racial resentment in a media market.
The expansion of the Sinclair Broadcasting Group during this period provides the basis for a difference-in-differences analysis estimating the effect of Sinclair Media purchasing a station on racial animus in an area.

I use two internet-based measures of racial animus, the number of google searches containing racial epiphets, and average scores on the IAT as measures of racial animus in an area.
Google trends data have previously been used to measure the contributions of racial resentment towards African-American mortality \parencites{Chae_2018}{Chae_2015}, election outcomes \parencite{Stephens_Davidowitz_2014}, and economic inequality \parencite[][]{Connor_2019}, and does not suffer from the same social censoring issues that confound traditional measures.

Overall, I find no link between Sinclair moving into a county and changes of racial resentment.
This result is robust to multiple measurement strategies and specifications.

\section{Theoretical Framework}

\section{Background}
\subsection{Sinclair Broadcast Group and Causal Inference}

In this paper, I use the expansion of the Sinclair Broadcasting Group from 2004-2020 as the basis of a difference-in-differences analysis.
Founded in 1971 as the Chesapeake Television Corporation, the Sinclair Broadcasting Group is one of the largest telecommunications providers in the U.S, and has grown to serve the maximum 39\% of houseolds allowable under U.S. law \parencite[][1]{Scherer_2018}.

Sinclair's pattern of expansion stands out from other providers in the industry.
Rather than purchasing stations in large media markets across the country, Sinclair has expanded by buying up stations in small media markets.
Previous research has demonstrated that Sinclair acquisition of a network is associated with a sharp rightwards shift in its coverage \parencite[][]{Martin_2019}.
I use the change in media coverage induced by Sinclair expansion as the basis of a difference-in-difference analysis of the contribution of media framing towards racial animus.

I create a record of Sinclair's station ownership over time from the company's yearly filings with the Securities and Exchange Commission, which enumerate all the stations owned by the company at the end of each year.
This strategy has previously been used by \citet{Miho_2018} to track Sinclair Expanison over the period of 1995-2017.
A map of the media markets Sinclair either purchased or sold a station in can be seen in in figure \ref{map}. A chart showing when each station became treated can be seen in \ref{treated}.

Traditionally, researchers track a network's expansion using datasets maintained by the Nielsen Corporation, the U.S. company responsible for tracking the boundaries of media markets.
The SEC dataset has an advantage over this dataset as it is open-access, and can be published freely alongside the research.

\begin{figure}
    \makebox[\linewidth][c]{%
        \begin{subfigure}[bt]{.6\textwidth}
            \centering
<<>>=
# Load and create data to show markets Sinclair has expanded to / moved out of
sinclair_expansions <- read_csv("../data/clean_sinclair_data.csv") %>%
    nest(-code) %>%
    mutate(
           any_true =  data %>% map_lgl(~any(.$sinclair_present)),
           any_false =  data %>% map_lgl(~any(!.$sinclair_present)),
           changed = any_true & any_false
    )


dma_boundaries <- st_read("../data/dma_boundaries/dma_boundary.shp")
dma_boundaries <- merge(dma_boundaries, sinclair_expansions, by.x="dma0", by.y="code")
@

\caption{Media Markets Sinclair Bought or Sold a Station in between 2004-2020. DMA Boundaries From \Cite[][]{Hill_2015}}
\label{map}
<<include = T>>=
#Plot markets Sinclair has expanded to / moved out of
ggplot() +
    geom_sf(data=dma_boundaries, aes(fill=changed), alpha=.5) +
    scale_fill_manual(values  = c(NA, "blue")) +
    theme_void() +
    theme(legend.pos ="none")
@
        \end{subfigure}%
    \begin{subfigure}[bt]{.6\textwidth}
        \centering
\caption{Media Markets by Sinclair Ownership Status, Time}
\label{treated}
<<include = T, warn = F >>=
#Plot markets Sinclair has expanded to / moved out of
sinclair_expansions %>%
    unnest(cols=c(data)) %>%
    unite("fill", changed:sinclair_present, remove = F)  %>%
    mutate(code = fct_reorder2(as.factor(code),as.factor(changed),as.factor(sinclair_present))) %>%
    ggplot(aes(year,code, fill=sinclair_present)) +
    geom_raster() +
    scale_y_discrete(breaks=c()) +
    scale_fill_discrete(labels = c("Untreated", "Treated")) +
    theme(legend.title = element_blank()) +
    xlab("Year") +
    ylab("Media Market")
@
    \end{subfigure}}\\
\end{figure}

\subsubsection{Does Sinclair Have a Racial Bias?}
I assume that Sinclair stations differ from non-Sinclair stations on coverage of racial issues: if Sinclair stations deploy the same coverage on racial issues as other stations, then there is no causal pathway by which Sinclair acquisition can change levels of racial resentment in a market.
In this section, I provide evidence to motivate this assumption.
While Sinclair's coverage of racial issues has not been extensively studied, research does show that Sinclair buying a station leads to a sharp rightwards shift in its coverage.
I argue that this effect extends to the coverage of racial issues - when Sinclair buys a station, it shifts the coverage of racial issues sharply towards the right, and provide anecdotal evidence to support this assertion.

Sinclair frequently pushes ``must run'' segments which are mandatory for owned stations to run.
They often take the form of scripts local anchors are mandated to read, or pre-filmed segments that are broadcast over all Sinclair statements.
Figure \ref{mustrun} shows images from one of these scripts being read on 30 stations and an accompanying transcript (ironically, a script about elite control of media institutions).


In 2010, Sinclair stations approved for broadcast and ran ``Breaking Point," an almost cartoonishly-racist, 25-minute campaign advertisement which, among other things, cast aspersions that Obama's 2008 presidential campaign was financed by Hamas, implied Obama was a Muslim by juxtaposing clips of him saying ``As-salamu alaykum'' (peace be with you) in a speech in Cairo against audio of Islamic prayers.

This year, the Sinclair corporation drew ire for a series of ``must-run'' segments on police violence following the murder of George Floyd pushing the ``black-on-black violence'' canard and advocating for a military response to the protests \parencites{Pleat_2020_b}{Pleat_2020_a}.


\begin{figure}
\label{mustrun}
\caption{Sinclair News Anchors Reading a ``Must-Run'' Script (May 2018)}
\include{media/must_run.tex}

\end{figure}

\subsection{Measuring Racial Animus}

Measuring racial resentment is difficult.
Overt expressions of racism are severly socially sanctioned in the U.S., so respondents who do harbor racial resentment often understate it when they are interviewed for traditional surveys \parencites{Krumpal_2011}{Kuklinski_1997}.
These social-desirability biases confound traditional survey measures, and lead to underestimates of the levels of racial hostility in the states.

% The Google search results provide higher external validity (almost all of the population is represented in Google search data) at the cost of low power (the data is variable, so small effects would be difficult or impossible to detect).
% I compliment this with data from Project Implicit which trades on external validity (IAT takers do not represent the general population) but the large sample size offers high power, so the chance of not finding a small effect is low.

I sidestep social desirability biases by using two unconventional measures of racial animus: the number of Google searches for racial slurs in an area, and test scores from Harvard University's Project Implicit, an internet-based project to collect data on implicit biases.
Both measures estimate racial animus from observed behavior (Google searches or the ability to associate black and white faces with positive words) rather than asking respondents questions, so sidestep issues of self-reporting.

\subsubsection{Google Trends Data}

I use the concentration of Google searches containing racial epithets as a proxy measurement of the level of racial animus in an area.
Data from Google searches are ideal are searches not subject to the social-desirability biases that confound traditional, survey-based measures of racial resentment, and provide a large, regularly-sampled source of data which represents a great deal of the population (at present, Google as an over 85\% market share in the US).

The core assumption of this strategy is that Google searches for the word \wone reflect underlying racial animus in an area.
If readers find this assumption unconvincing, then they will have little reason to accept the conclusions of this analysis.
Accordingly, I offer several reasons to suggest that searches for racial epithets well proxy racial animus in an area, namely: searches are conducted in private, and are not subject to the same social-desirability biases as traditional, survey-based measures; and measurements of area-level racism obtained from Google trends data correspond well to traditional survey-based and non-survey-based measures of racial animus.

First, Google searches do not suffer from the same social censoring as traditional measures of public opinion that rely on face-to-face or over-the-phone interviews.
Overt expressions of racism are no longer socially palatable in the U.S.,
Social desirability bias is a pressing concern when it comes to measuring racial animus; research has repeatedly demonstrated that \parencite[][]{Kuklinski_1997}

This measurement approach has an advantage over traditional survey-based measures of racial animus in that it is less subject to a social desirability bias;
``Google searchers are online and likely alone, both of which make it easier to express socially taboo thoughts (Kreuter et al., 2009)" \parencite[][26]{Stephens_Davidowitz_2014}.
Further, it provides a high-resolution set of data that would be prohibitively expensive to collect from a traditional survey, especially given that the difference-in-differences approach requires a repeated survey comparable across multiple time periods.
\footnote{\cite{Stephens_Davidowitz_2014} evidences this claim by reporting statistics for pornography searches. Over the past 16 years, the number of searches for ``porn" and ``news" are commensurate, yet only 14\% of GSS respondents tell the GSS they have visited a pornographic website in the past 30 days.}

%As such, I give several reasons to suggest that Google trends data can be used to measure racial animus.

%Originally developed by \citet{Stephens_Davidowitz_2014} to understand the link between racial animus and the under-performance of black candidates in national elections.
%This measure has significant advantages over traditional survey-based methods as it is not subject to the same social desirability biases that confound traditional measures; and provides a rich, regularly-sampled repeated measure of racial animus.

A pressing concern is that searches for  the word ``\wone'' might not actually measure racial animus but, in fact, reflect users learning about the term.
Indeed, ``Definition of \wone'' and ``what does \wone mean" are both among the top 5 search queries related to term.
These queries suggest that many who search for the term are searching out of curiosity to investigate the term.

This is not to suggest that searches for the term do not capture any variation in racial animus: the top 10 most searched related queries to the term \wones are ``I don't like \wones,"  ``fuck the \wones," and ``ship those \wones back."
\citet{Stephens_Davidowitz_2014} evidences this claim by reporting strong correlations between opposition to interracial marriage and Google searches for \wone at the state level.
I provide further support for this claim by reporting correlations between searches for \wone and the scores of white respondents in a state on the IAT.
Following practices of other research using IAT data \parencite[][206]{Connor_2019} I use the scores of only white respondents.


<<include=T, warn=FALSE>>=
library(ggrepel)

state_iat_data <- read_csv("../data/iat_state_data.csv")
word1_state_data <- read_csv("../data/word1_all_time.csv")

state_iat_searches <- full_join(state_iat_data,word1_state_data) %>%
    mutate(sbp5 = 5 - sbp5) %>%
    drop_na()
@


\begin{figure}
\ref{correlations}
    \makebox[\linewidth][c]{%
        \begin{subfigure}[bt]{.6\textwidth}
            \centering
<<include=T>>=
ggplot(state_iat_searches, aes(word1, iat, label=state)) +
    geom_text_repel() +
    geom_smooth(method="lm") +
    xlab("Frequency of Google Searches for [Word 1]") +
    ylab("Scaled IAT Score of White Respondents")
@
        \end{subfigure}%
    \begin{subfigure}[bt]{.6\textwidth}
        \centering
<<include=T>>=
ggplot(state_iat_searches, aes(word1, sbp5, label=state)) +
    geom_text_repel() +
    geom_smooth(method="lm") +
    xlab("Frequency of Google Searches for [Word 1]") +
    ylab("Scaled Responses to Prompt 5")
@
    \end{subfigure}}\\
\end{figure}

\subsubsection{Scaling Google Trends Data:}

Google trends data measures the popularity of a search on an idiosyncratic scale: a term's ``Google search score" in a given time is given as the volume of searches over that time divided by the volume of searches when the term was most popular.
However, the difference-in-difference analysis I perform requires that the search volume is measured on the same scale.
In this section, I describe the scaling process used to back out an interval-level measure of search popularity from these search scores.

Search popularity can be obtained from Google in two flavors: a measure that compares the popularity of a search across all regions at a given time, and a measure that gives the popularity of a search in a given region across all times.

The search score that can be used to make comparisons between regions at a given time is defined as the following:

\[
    \text{Between Regions Search Score} = \frac{\text{Popularity in Region } i }{\text{Popurality in the Most Popular Region}}
\]

The search score that can be used to compare the popularity across different times in a given region is defined as the following.
\[
    \text{Between Times Search Score} = \frac{\text{Popularity at Time } i }{\text{Popurality at the Most Popular Time}}
\]

The first measure allows for comparisons between regions but not between times, while the second permits comparisons between times but only within one region.
When combined together, these measures can be used to compare the volume of searches across both time periods and regions.

As an example, we might want to compare searches for the word apple in Washington D.C. in 2017 to searches in Pensacola in 2018.
Washington D.C. had twice the number of searches for the word apple as Pensacola in 2017. As Pensacola had three times the amount of searches for the word apple in 2017 as 2018, we know that Pensacola had just $1/6th$ the searches for the word apple in 2018 as Washington D.C. in the year 2017.

By comparing the volume of searches in each region at each time to each other region, I back out a measure of search volume, $v$ that is comparable both between regions and between times.

\[
    v_{rt} \propto
    \mathlarger{\mathlarger{\mathlarger{\sum}}}_{i=1}^{I}
    \mathlarger{\mathlarger{\mathlarger{\sum}}}_{j=1}^{J}
    \frac{v_{rt}}{v_{ij}}
\]

For every $v_{rt} \neq v_{i'j'}$, where:
\begin{itemize}
    \item $v_{ij}$ Represents the search volume in region i at time j
    \item Regions $j \in (1\dots J)$
    \item Time periods $i \in  (1\dots I)$
\end{itemize}

\begin{figure}
\caption{Illustration of Scaling Algorithm}
\include{media/scaling.tex}
\end{figure}

\subsection{Implicit Association Test data}


\begin{figure}
\includegraphics[width=.95\textwidth]{raceiat.png}
\caption{Slides From an IAT test \parencite[][]{iat_2021}}
\label{slides}
\end{figure}

To provide a second measure of racial animus, I use data from the Harvard race IAT \parencite[][]{iat_2021}.
The Harvard IAT is an internet-based quiz founded in 1998 that seeks to measure implicit associations between ``concepts (e.g., black people, gay people) and evaluations (e.g., good, bad)''  \parencite[][]{iat_web}.

The race IAT asks participants are sort good and bad words, and African-American and European-American into two categories.
Examples of these faces and words can be seen in figure \ref{slides}.
The IAT records the difference in the speed with which respondents can sort European-American faces with good words / African-American faces with bad words, and European-American faces with bad words / African-American faces with good words.

The IAT records the differences in the speed with which respondents can sort European-American + Good / African-American + Bad together and European-American + Bad / African-American + Good together.
The IAT would record a positive score (an implicit preference for European-Americans) if a respondent was quicker to code European-American + Good / African-American + Bad together than they were to code European-American + Bad / African-American + Good together.
The underlying logic behind using the differences in speeds is that ``making a response is easier when closely related items share the same response key [category]" \parencite[][]{iat_2021}, so respondents who have a strong implicit association between African-American and Bad will find it more difficult to code European-American and Good together.

The IAT responses are not subject to social-desirability biases as they are determined by implict associations that respondents cannot control.
In fact, methodological studies have shown that even when asked to create a certain results, participants are unable to devise or enact a strategy to get their desired results unless given specific instructions on how to do so by a researcher \parencite[][88-91]{Kim_2003}.

Data from the IAT is recorded at the county level, I measure racial animus at the larger DMA level.
To solve this issue, I aggregate IAT scores to the DMA level using a crosswalk of US counties to DMA ID's from 2016 \parencite[][]{Guarv_2016}.
In actual fact, DMA boundaries undulate slightly over time to include and exclude new counties, and a lack of publicly-accessible historical data means I cannot account for these movements.
This is an issue shared by other research on media institutions that aggregate county-level data to the DMA level \parencite[][9]{Miho_2018}, and leads to a slight underestimation of the treatment effect, as a small number if untreated units will be classified as treated and vice-versa.

\section{Methods}

\subsection{Tools Used}

I use Knitr \parencite[][]{knitr} to integrate statistical calculations into the paper, eliminating the possibility of transcription errors.
To ensure that the methods the findings are reproducible, I tested the analysis routines using the \textit{testthat} package in R \parencite[][]{testthat} and the \textit{unittest} module in Python \parencite[][]{Python}.
But I won't make you take my word for it – I provide a Docker image with the reproducibility materials to ensure others can replicate the calculations on their own systems \parencites{docker}{Boettiger_2015}.
The net result is ``one-click reproducibility" \parencite[][]{N_st_2020}; readers can reproduce this exact paper with the push of a button from the linked materials. \footnote{Replication materials available \href{https://github.com/beniaminogreen/dissertation}{here}.  By default, the web-scraping does not run, as the data take several days to collect.}

\subsection{Preregistration}

To avoid the possibility of fitting hypotheses to the data after results are known, I created a preregistration plan of my analysis using the Google trends data. The plan can be seen in section \ref{prereg}.
As I decided to conduct the analysis using the project implicit data to confirm the Google trends results, I did not preregister this analysis plan.

I have made one deviation from the preregistration.
In my preregistration, I describe a different strategy to scale the Google trends data than the one I actually employ.
This original strategy is based on a misunderstanding of the format of Google trends data, and does not actually produce the desired measure.

In the analysis I perform, I correct this mistake. I describe the correct scaling procedure in section \ref{hey}.





\section{Results}

<<>>=

load("../data/models.Rdata")

@
<<include=TRUE, results="asis">>=
stargazer(model_1, model_2, model_4, model_5,
          omit=c('^as\\.factor\\(year\\)[0-9]{4}$',
                 "^code[0-9]{3}$",
                 "^code[0-9]{3}\\:year$",
                 "code"
                 ),
	title = "Fixed Effects Models For Sinclair Aqusisition on Google Searches",
	dep.var.labels = c("Frequency of Searches for \\wone", "Frequency of Searches for Words 1-5"),
	covariate.labels = c("Sinclair Present", "Constant"),
	add.lines =
        list(
             c("Year Fixed Effects", "Yes", "Yes", "Yes", "Yes"),
             c("Region Fixed Effects", "Yes", "Yes","Yes", "Yes"),
             c("Region Time Trends", "No", "Yes", "No", "Yes")),
        omit.stat=c("LL","ser","f"),
           table.placement="H")
@

<<>>=
library(biglm)
library(texreg)
load("../data/models.Rdata")
@

<<include=TRUE, results="asis">>=
texreg(list(model_7, model_8, model_10, model_11),
       custom.gof.rows =
           list(
                "Year Fixed Effects" = c("Yes", "Yes", "Yes", "Yes"),
                "Region Fixed Effects" = c("Yes", "Yes", "Yes", "Yes"),
                "Region Time Trends" = c("No", "Yes", "No", "Yes"),
                "White Respondents Only" = c("No", "No", "Yes", "Yes")
                ),
       float.pos = "H",
       omit.coef="code|year")
@


\subsection{Identification Assumption}

In this section, I test the indication assumption, the assumption that the treated and control units would have the outcomes if the treatment were absent.


\[
    \text{Racially Charged Search Rate} = \beta_{1}(\text{ Sinclair Present }) + \beta_{2}(\text{ DMA fixed effects }) + \beta_{3}(\text{ year fixed effects }) +
    \beta_{4}(\text{Year / DMA fixed effects})
\]

\begin{centerfig}
\caption{Fixed-Effects Estimates of the effect of Sinclair Acquisition on Searches for ``\wone"}
<<include =T, warn = FALSE >>=
model_3 %>%
  tidy() %>%
  na.omit() %>%
  filter(grepl("years_before", term)) %>%
  mutate(term = as.numeric(gsub("[^0-9\\-]+", "", term))) %>%
  ggplot(aes(x = term, y = estimate)) +
  geom_point() +
  geom_errorbar(aes(ymin = estimate - 1.96 * std.error, ymax = estimate + 1.96 * std.error)) +
  geom_hline(aes(yintercept=0), linetype=2) +
  geom_vline(aes(xintercept=0)) +
  xlab("Years Relative to Sinclair Acquisition") +
  ylab("Estimated Effect of Acquisition on Number of Searches for \"[Word 1]\"")
@
\end{centerfig}

\newpage
\appendix{}
\section{Codings for Offensive Words}
\begin{figure}
\begin{table}[H]
\centering
        \begin{tabular}{l|l}
\label{words}
            Code & Word \\ \hline
            Word 1  & Nigger   \\ \hline
            Word 2 & Coon \\ \hline
            Word 3 & Kike \\ \hline
            Word 4 & Spic \\ \hline
            Word 5 & Spook \\ \hline
        \end{tabular}
\end{table}
\end{figure}
\printbibliography{}

\section{Code In This Document}
<<echo=FALSE, include=T>>=
# Pull R code out of this document to put into the appendix
code <- knitr::purl("diss.Rnw", quiet=T)
@

\section{Web Scraping Code}
% \subsection{Master Web Scraping Script}
% \lstinputlisting[language=python]{../code/web_scrape.py}
% \subsection{Collecting Data for Between-Region Comparisons}
% \lstinputlisting[language=python]{../code/between_regions.py}
% \subsection{Collecting Within-Region Data}
% \lstinputlisting[language=python]{../code/in_region.py}
% \subsection{Utility Functions}
% \lstinputlisting[language=python]{../code/utils.py}
% \section{Analysis Code}
% \lstinputlisting[language=R]{../code/analysis.R}
% \subsection{Utility Functions}
% \lstinputlisting[language=R]{../code/utils.R}
% \section{Unit tests}
% \subsection{For Python Code}
% \lstinputlisting[language=python]{../code/test_between_regions.py}
% \lstinputlisting[language=python]{../code/test_in_region.py}
% \lstinputlisting[language=python]{../code/test_scaling.py}
% \lstinputlisting[language=python]{../code/test_utils.py}
% \subsection{For R Code}
% \lstinputlisting[language=R]{../code/tests/testthat/test_utils.R}

\end{document}
