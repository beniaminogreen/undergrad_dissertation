\documentclass{article}

\usepackage{setspace}
\usepackage[colorlinks, allcolors=blue]{hyperref}
\usepackage{graphicx}
\usepackage{pdfpages}
\usepackage{float}
\usepackage{chngpage}
\usepackage{mathtools}
\usepackage{subcaption}
\usepackage{framed}
\usepackage{listings}
\usepackage{amssymb}
\usepackage{tikz}
\usetikzlibrary{positioning,arrows.meta}

\definecolor{arrowblue}{RGB}{98,145,224}

\newcommand\ImageNode[3][]{
  \node[draw=arrowblue!80!black,line width=1pt,#1] (#2) {\includegraphics[width=3.5cm,height=3.5cm]{#3}};
}
\graphicspath{ {./media/} }
\usepackage[backend=biber, style=authoryear-icomp,doi=false,isbn=false,url=false]{biblatex}
\addbibresource{$BIB}

\definecolor{codegreen}{rgb}{0,0.6,0}
\definecolor{codegray}{rgb}{0.5,0.5,0.5}
\definecolor{codepurple}{rgb}{0.58,0,0.82}
\definecolor{backcolour}{rgb}{0.95,0.95,0.92}

\lstdefinestyle{mystyle}{
    backgroundcolor=\color{backcolour},
    commentstyle=\color{codegreen},
    keywordstyle=\color{magenta},
    numberstyle=\scriptsize\color{codegray},
    stringstyle=\color{codepurple},
    basicstyle=\ttfamily\scriptsize,
    breakatwhitespace=false,
    breaklines=true,
    captionpos=b,
    keepspaces=true,
    numbers=left,
    numbersep=5pt,
    showspaces=false,
    showstringspaces=false,
    showtabs=false,
    tabsize=2
}
\lstset{style=mystyle}

\addtolength{\oddsidemargin}{-.5in}
\addtolength{\evensidemargin}{-.5in}
\addtolength{\textwidth}{1in}
\addtolength{\topmargin}{-.5in}
\addtolength{\textheight}{1in}

\newcommand{\wone}{[Word 1] }
\newcommand{\wones}{[Word 1]'s }
\newcommand{\woneps}{[Word 1](s) }

\newenvironment{centerfig}
{\begin{figure}[H]\centering}
{\end{figure}}


<<echo=FALSE, warn=FALSE, include=FALSE>>=
# Set options to knit this document, load libraries
knitr::opts_chunk$set(out.width="100%", message=F, cache=T,echo=F, warn=F, include=F)
library(tidyverse)
library(sf)
library(broom)
library(stargazer)
@

\begin{document}
\title{}
\maketitle{}
\textit{This paper uses Google search terms containing offensive language as a proxy measure for racial animus. Following the practice of similar works \parencites{Stephens_Davidowitz_2014}{Chae_2015}{Chae_2018}{Isoya_2021}, I use coded language to refer to these terms. The words themselves can be found in Table \ref{words}}

\section{Introduction}

\section{Methods}

\subsection{Tools Used}

Surveys of major social science journals routinely fail to reproduce the findings of a plurality or majority of papers from the supplementary code and data provided \parencites[][]{Nuijten_2015}{Nuijten_2020}{Eubank_2016}.

Failures to replicate are often due to coding errors or mistakes in transcribing the results of a calculation into a published manuscript \parencite[][276]{Eubank_2016}.

I use Kintr to integrate statistical calculations into the paper, eliminating the possibility of transcription errors \parencite[][]{knitr}.
To ensure that the methods of this paper have been properly implemented and the finding are reproducible, I tested the analysis routines using the \textit{testthat} package in R \parencite[][]{testthat} and the \textit{unittest} module in Python \parencite[][]{Python}.  But I won't make you take my word for it  â€“
I provide a Docker image with the reproducibility materials to ensure others can replicate the calculations on their own systems \parencites{docker}{Boettiger_2015}.
The net result is ``one-click reproducibility" \parencite[][]{N_st_2020}; readers can reproduce this exact paper with the push of a button from the linked materials.
\footnote{Replication materials available \href{https://github.com/beniaminogreen/dissertation}{here}}

\subsection{Preregistration}

To avoid the possibility of fitting hypotheses to the data after results are known, I created a preregistration plan of my analysis. The plan can be seen in section \ref{prereg}.

I have made one significant deviation from the preregistration plan.
In my preregistration, I describe a strategy to back out a ratio-level measure of the number of searches for \wone in an area from Google trends data.
This strategy is based on a misunderstanding of the format of Google trends data, and does not actually produce the desired measure.

In the analysis I perform, I correct this mistake. I describe the correct scaling procedure in section \ref{hey}.

\begin{figure}
\caption{Illustration of Scaling Algorithm}

\begin{tikzpicture}[
  node distance=1.5cm,
  >={Triangle[angle=60:1pt 2]},
  shorten >= 2pt,
  shorten <= 2pt,
  arrow/.style={
    ->,
    arrowblue,
    line width=5pt
  }
]
\ImageNode[label={-90:Scaled}]{gen1}{gen1}
\ImageNode[label={Horizontally Comparable Matrix},above left=of gen1]{hmatrix}{hmat}
\ImageNode[label={-90:Vertically Comparable Matrix},below left=of gen1]{vmatrix}{vmat}
\ImageNode[label={-90:Repaired Dataset},right=of gen1]{repaired}{repaired}

 \draw[arrow]
   (hmatrix) -- (gen1);
 \draw[arrow]
   (vmatrix) -- (gen1);
 \draw[arrow]
   (gen1) -- (repaired);
\end{tikzpicture}
\end{figure}


\begin{figure}
\caption{Sinclair News Anchors Reading a ``Must-Run'' Script (May 2018)}
\include{media/must_run.tex}
\end{figure}

<<>>=
# Load and create data to show markets Sinclair has expanded to / moved out of
sinclair_expansions <- read_csv("../data/clean_sinclair_data") %>%
    nest(-code) %>%
    mutate(
           any_true =  data %>% map_lgl(~any(.$sinclair_present)),
           any_false =  data %>% map_lgl(~any(!.$sinclair_present)),
           changed = any_true & any_false
    )

dma_boundaries <- st_read("../data/dma_boundaries/dma_boundary.shp")
dma_boundaries <- merge(dma_boundaries, sinclair_expansions, by.x="dma0", by.y="code")
@

\begin{centerfig}
<<include = T>>=
# Plot markets Sinclair has expanded to / moved out of
ggplot() +
    geom_sf(data=dma_boundaries, aes(fill=changed), alpha=.5) +
    scale_fill_manual(values  = c(NA, "blue")) +
    ggtitle("Map of Sinclair Stations That Bought or Sold in 2004-2021") +
    theme_void()
@
%\caption{Media Markets Sinclair Bought or Sold a Station in 2004-2001 DMA Boundaries From \Cite[][]{Hill_2015}}
\end{centerfig}

\section{Results}

<<>>=
full_data <- read_csv("../data/full_data.csv") %>%
    mutate(code = as.factor(code))

model_1 <- lm(sword1 ~ as.factor(year) + code + sinclair_present, data = full_data)

model_2 <- lm(sword1 ~ as.factor(year) + code + code:year+ sinclair_present, data = full_data)

model_3 <- lm(sword1 ~ as.factor(year) + as.factor(code) + as.factor(years_before), data = full_data)

@


\[
    \text{Racially Charged Search Rate}= \beta_{1}(\text{ Sinclair Present }) + \beta_{2}(\text{ DMA fixed effects }) + \beta_{3}(\text{ year fixed effects })
\]



<<include=TRUE, results="asis">>=
stargazer(model_1, model_2,
          omit=c('^as\\.factor\\(year\\)[0-9]{4}$',
                 "^code[0-9]{3}$",
                 "^code[0-9]{3}\\:year$"
                 ),
	title = "Fixed-Effect Model Results",
	dep.var.labels = c("Frequency of Google Searches for \\wone"),
	covariate.labels = c("Sinclair Present", "Constant"),
	add.lines =list(c("Year Fixed Effects", "Yes", "Yes"),c("Region Fixed Effects", "Yes", "Yes"), c("Region / Year Fixed Effects", "Yes", "No")),
           table.placement="H")
@

\subsection{Indentification Asssumption}

In this section, I test the indefication assumption, the assumption that the treated and control units would have the outcomes if the treatment were absent.


\[
    \text{Racially Charged Search Rate} = \beta_{1}(\text{ Sinclair Present }) + \beta_{2}(\text{ DMA fixed effects }) + \beta_{3}(\text{ year fixed effects }) +
    \beta_{4}(\text{Year / DMA fixed effects})
\]

\begin{centerfig}
\caption{Fixed-Effects Estimates of the effect of Sinclair Acquisition on Rate of Racially Charged Google Searches}
<<include =T, warn = FALSE >>=
model_3 %>%
  tidy() %>%
  na.omit() %>%
  filter(grepl("years_before", term)) %>%
  mutate(term = as.numeric(gsub("[^0-9\\-]+", "", term))) %>%
  ggplot(aes(x = term, y = estimate)) +
  geom_point() +
  geom_errorbar(aes(ymin = estimate - 1.96 * std.error, ymax = estimate + 1.96 * std.error)) +
  geom_hline(aes(yintercept=0), linetype=2) +
  geom_vline(aes(xintercept=0)) +
  xlab("Years Relative to Sinclair Acquisition") +
  ylab("Estimated Effect of Acquisition on Number of Racially-Charged Google Searches")
@
\end{centerfig}


\begin{centerfig}
\caption{Fixed-Effects Estimates of the effect of Sinclair Acquisition on Rate of Racially Charged Google Searches [Larger Bins]}
<<include =T, warn = FALSE >>=
full_data <- full_data %>%
    mutate(years_before_5 = cut(years_before,c(-100,seq(-15,25,5)),right=F))

model_4 <- lm(sword1 ~ as.factor(year) + as.factor(code) + as.factor(years_before_5), data = full_data)

model_4 %>%
  tidy() %>%
  na.omit() %>%
  filter(grepl("years_before", term)) %>%
  mutate(term = gsub(".*\\[", "[", term)) %>%
  ggplot(aes(x = term, y = estimate)) +
  geom_point() +
  geom_errorbar(aes(ymin = estimate - 1.96 * std.error, ymax = estimate + 1.96 * std.error)) +
  geom_hline(aes(yintercept=0), linetype=2) +
  geom_vline(aes(xintercept="[0,5)"),linetype=2) +
  xlab("Years Relative to Sinclair Acquisition") +
  ylab("Estimated Effect of Acquisition on Number of Racially-Charged Google Searches")

@
\end{centerfig}



\newpage
\appendix{}
\section{Codings for Offensive Words}
\begin{figure}
\label{words}
\begin{table}[H]
\centering
        \begin{tabular}{l|l}
            Code & Word \\ \hline
            Word 1  & nigger   \\ \hline
        \end{tabular}
\end{table}
\end{figure}
\printbibliography{}

\section{Code In This Document}
<<echo=FALSE, include=T>>=
# Pull R code out of this document to put into the appendix
code <- knitr::purl("diss.Rnw", quiet=T)
@

\section{Web Scraping Code}
\subsection{Master Web Scraping Script}
\lstinputlisting[language=python]{../code/web_scrape.py}
\subsection{Collecting Data for Between-Region Comparisons}
\lstinputlisting[language=python]{../code/between_regions.py}
\subsection{Collecting Within-Region Data}
\lstinputlisting[language=python]{../code/in_region.py}
\subsection{Utility Functions}
\lstinputlisting[language=python]{../code/utils.py}
\section{Analysis Code}
\lstinputlisting[language=R]{../code/analysis.R}
\subsection{Utility Functions}
\lstinputlisting[language=R]{../code/utils.R}
\section{Unit tests}
\subsection{For Python Code}
\lstinputlisting[language=python]{../code/test_between_regions.py}
\lstinputlisting[language=python]{../code/test_utils.py}
\subsection{For R Code}
\lstinputlisting[language=R]{../code/tests/testthat/test_utils.R}

\end{document}
